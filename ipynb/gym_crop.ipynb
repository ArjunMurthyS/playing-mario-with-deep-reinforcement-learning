{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state, reward, done, info = env.step(action=env.action_space.sample())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 84, 84\n",
    "\n",
    "def downsample(frame: np.ndarray, x: int=8, y: int=14) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Down-sample the given frame from RGB to B&W with a reduced size.\n",
    "\n",
    "    Args:\n",
    "        frame: the frame to down-sample\n",
    "\n",
    "    Returns:\n",
    "        a down-sample B&W frame\n",
    "\n",
    "    \"\"\"\n",
    "    # convert the frame from RGB to gray scale\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # crop the image\n",
    "    frame = frame[2*y:frame.shape[0] - y, x:frame.shape[1] - x]\n",
    "    \n",
    "    # zero out specific colors\n",
    "    # 142 is the generic gray color\n",
    "    frame[frame == 142] = 0\n",
    "\n",
    "    # resize the frame to the expected shape\n",
    "    frame = cv2.resize(frame, image_size)\n",
    "    # normalize the image to floating point in [0, 1]\n",
    "    frame = frame / 255.0\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_image(image: np.ndarray, channel_range: tuple=(0, 255)) -> Image:\n",
    "    \"\"\"\n",
    "    Convert the input matrix to an image.\n",
    "    Args:\n",
    "        image: the matrix of shape [height, width, channel] to convert\n",
    "        channel_range: the range to clip the channel values to (inclusive)\n",
    "    Returns:\n",
    "        an image from the pixels in the image array\n",
    "    \"\"\"\n",
    "    # clip the values in the image to the boundary [0, 255]. This is the\n",
    "    # legal range for channel values. Image uses a method called 'to bytes'\n",
    "    # to compress the input array into a simpler binary representation for\n",
    "    # graphics processing. As such, convert the type to a single byte to\n",
    "    # satisfy this constraint.\n",
    "    image = np.clip(image, *channel_range).astype('uint8')\n",
    "\n",
    "    return Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFQAAABUCAAAAAA5AE8dAAAAHklEQVR4nO3BAQ0AAADCoPdPbQ43\noAAAAAAAAIAnAxvkAAFTUKNRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=84x84 at 0x120CC0B70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_to_image(downsample(next_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAA+RJREFUeJzt3bFt1GAYgGEbpUZMQEXBCBEDRClYJpmACTIGYgCKiIIS\nZRhEgRBFipj+pBAnr4nt43lq339/8953/hVfxmmaBuDpXqy9Adg7EUEkIohEBJGIIBIRRCKCSEQQ\niQiik7U3MAzDMI6jP5tgc6ZpGudcZxJBJCKIRASRiCDaxMHCFl1dXT36NZeXl2mNw9cvtUa1hT0c\nOtzTc7znfUwiiEyimf7FlFhj2j3FQ5/yz7GHLTOJIDKJeLT/ffIcMokgMol4tDXuy7bMJILIJJpp\niU/brayxh/fcE5MIIhFBNG7hF1A9T8QWeZ4InskmDhbcuLJnJhFEIoJIRBCJCCIRQSQiiEQEkYgg\nEhFEIoJIRBCJCCIRQSQiiDbxKMRD1vydZY7XUo/gmEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgi\nEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQ7eIXUG/Oz9feAkfo20LrmEQQ\niQgiEUEkIohEBNEuTufu3vxcewtwL5MIIhFBJCKIRASRiCASEUS7OOL+8fL32luAe5lEEIkIIhFB\nJCKIRATRPk7n3t6uvQWO0fdlljGJIBIRRCKCSEQQiQiiXZzOfbx7vfYWOEJnC61jEkEkIohEBJGI\nIBIRRLs4nbv99GHtLXCMzpb55yomEUQigkhEEIkIIhFBJCKIdnHE/fX6dO0tcITen10tso5JBJGI\nIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEk\nIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQ\niQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIR\nRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIBIRRCKCSEQQiQgiEUEkIohE\nBJGIIBIRRCKCSEQQiQgiEUEkIohEBJGIIDpZewPDMAyfX/2add3N+Xl+r9Pr67wGx+Hdly9/v+Di\nYtY6JhFEIoJIRBBt4p5oLvczbJFJBNGuJhEs6aFvNtPMdcZpmnvpvzOO4/qbgAPTNI1zrvN1DiIR\nQSQiiEQEkYggEhFEIoJIRBCJCCIRQSQiiEQEkYggEhFEm3gUAvbMJIJIRBCJCCIRQSQiiEQEkYgg\nEhFEIoJIRBCJCCIRQSQiiEQEkYggEhFEIoJIRBCJCCIRQSQiiEQEkYgg+gNZx2HEjttKlgAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12257ee80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    next_state, reward, done, info = env.step(action=env.action_space.sample())\n",
    "    show_state(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
