
# Abstract

*   comparable results to human performance on image recognition and NLP
	*   evolution of **topology**, **components**, and **hyperparameters**
		*   i.e. _number of layers_, _units per layer and their design_, and
			_the final set of tuning parameters_
*   given more computational power, this method _could_ be competitive in
	deep learning

> The success of deep learning depends on finding an architecture to t the
task. As deep learning has scaled up to more challenging tasks, the
architectures have become difficult to design by hand. is paper proposes an
automated method, CoDeepNEAT, for optimizing deep learning architectures
through evolution. By extending existing neuroevolution methods to topology,
components, and hyperparameters, this method achieves results comparable to
best human designs in standard benchmarks in object recognition and language
modeling. It also supports building a real-world application of automated
image captioning on a magazine website. Given the anticipated increases in
available computing power, evolution of deep networks is promising approach to
constructing deep learning applications in the future.
