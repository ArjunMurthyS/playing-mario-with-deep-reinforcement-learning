# Results

We combine both \ac{DDQN} and Double Deep-$Q$ to produce the Double Dueling
Deep-$Q$ Network. We use the hyperparameters in Table
\ref{tab:hyperparameters}. Because of issues with the \ac{NES} emulator
crashing during training, we were unable to keep track of the number of
training frames $T$ and performance metrics such as the loss and reward of
each training episode. We can however, validate an untrained and trained
agent to confirm learning.
